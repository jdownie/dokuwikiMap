#!/usr/bin/env python3

import copy
import json
from pprint import PrettyPrinter
import requests
from bs4 import BeautifulSoup

index = dict()
root = "https://wiki.hsbne.org"
pp = PrettyPrinter(indent=2)

def dump():

  fout = open("static/wiki.json", "w")
  fout.write(json.dumps(index))
  fout.close()

def scan(root, page):
  
  ret = list()
  url = "{0}{1}".format(root, page)
  html = requests.get(url).text
  tokens = [ "?", "#" ]
  ignore = [ "jpeg", "jpg", "pdf", "php", "png", "zip", "docx", "dxf", "gg", "gif", "io" ]
  soup = None
  try:
    soup = BeautifulSoup(html, "html.parser")
  except:
    None
  if soup != None:
    nodes = soup.find_all("a")
    for node in nodes:
      page = node.get("href")
      if page != None and not page in ret and page[0:1] == "/":
        for token in tokens:
          parts = page.split(token)
          page = parts[0]
        parts = page.split(".")
        ext = parts.pop()
        if not ext in ignore and not page in ret:
          ret.append(page)
  return ret

def digest(page, index):
  if not page in index.keys():
    pages = scan(root, page)
    index[page] = copy.deepcopy(pages)
    print("Found {0} pages in {1}".format(len(pages), page))
    while len(pages) > 0:
      page = pages.pop()
      digest(page, index)
  dump()
  return index

index = digest("/", index)
pp.pprint(index)

#x = scan(root, "/tools/home")
#pp.pprint(x)
